{
  "_documentation": {
    "description": "Configuration options for the AI Agent service. All values can be overridden via environment variables. API keys (LANGCHAIN_API_KEY, OPENAI_API_KEY) must be set via environment variables only.",
    "models": "Array of model configurations. Each model object must have a 'type' field (any string, e.g., 'ollama', 'openai', 'Google') and type-specific properties.",
    "langchain_tracing_v2": "Enable LangSmith tracing. Set to 'true' or 'false'",
    "langchain_project": "LangSmith project name for organizing traces",
    "langchain_endpoint": "Custom LangSmith endpoint URL (optional). Leave empty for default",
    "use_embeddings": "Enable embedding-based search. Set to true or false",
    "embedding_model": "Model name for embeddings. Example: 'text-embedding-3-small', 'text-embedding-3-large'",
    "embedding_top_k": "Number of top results to return from embedding search (integer)",
    "fetch_url": "URL for the search/fetch service endpoint",
    "profile_url": "Base URL for the profile service",
    "server_host": "Host address for the agent server. Use '0.0.0.0' to accept connections from any interface",
    "server_port": "Port number for the agent server (integer)",
    "cors_origins": "Array of allowed CORS origins. Add frontend URLs that should be allowed to access the API",
    "enable_streaming": "Enable real-time token streaming. Set to true or false (default: false)"
  },
  "agent_config": {
    "models": [
      {
        "type": "ollama",
        "model": "llama3:latest",
        "temperature": 0.7,
        "max_tokens": null,
        "base_url": "http://localhost:11434"
      },
      {
        "type": "openai",
        "model": "gpt-5-mini",
        "temperature": 0.7,
        "max_tokens": null,
        "base_url": ""
      }
    ],
    "langchain_tracing_v2": "true",
    "langchain_project": "chat-agent",
    "langchain_endpoint": "",
    "use_embeddings": true,
    "embedding_model": "text-embedding-3-small",
    "embedding_top_k": 5,
    "fetch_url": "http://localhost:8090/search/smart",
    "profile_url": "http://localhost:8080",
    "server_host": "0.0.0.0",
    "server_port": 8070,
    "enable_streaming": true,
    "cors_origins": [
      "http://localhost:5173",
      "http://localhost:3000"
    ]
  }
}

